# PAIR-Adversarial-LLM ğŸ”ğŸ¤–

This project implements **PAIR (Prompt Automatic Iterative Refinement)** to generate adversarial jailbreak prompts against LLMs like GPT-4, Claude, and Gemini, and evaluates their defenses.

---

## ğŸ“ Project Structure

- `attacker/` â€“ Logic to generate and refine adversarial prompts
- `target_models/` â€“ Interfaces for OpenAI, Anthropic, Gemini, and local models
- `evaluator/` â€“ Safety classifier, evaluation metrics
- `data/` â€“ Prompts, jailbreaks, logs
- `scripts/` â€“ Main entry points for running attack or evaluation
- `notebooks/` â€“ Visualizations and debugging
- `tests/` â€“ Unit tests

---

## ğŸš€ Getting Started

1. **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2. **Run PAIR attack:**
    ```bash
    python run_pair.py --config configs/attacker_config.yaml
    ```

3. **Evaluate generated jailbreak prompts**
    ```bash
    python eval_jailbreaks.py
    ```

4. **Launch the Streamlit Dashboard**
    ```bash
    streamlit run app.py
    ```

---

